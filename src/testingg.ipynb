{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Epoch 1/50\n",
      "448/448 [==============================] - 301s 668ms/step - loss: 1.7978 - accuracy: 0.2623 - val_loss: 1.6986 - val_accuracy: 0.3394\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 286s 639ms/step - loss: 1.6247 - accuracy: 0.3707 - val_loss: 1.5447 - val_accuracy: 0.4015\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 284s 634ms/step - loss: 1.5215 - accuracy: 0.4142 - val_loss: 1.4575 - val_accuracy: 0.4459\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 283s 633ms/step - loss: 1.4528 - accuracy: 0.4463 - val_loss: 1.4083 - val_accuracy: 0.4639\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 283s 633ms/step - loss: 1.3985 - accuracy: 0.4665 - val_loss: 1.3476 - val_accuracy: 0.4890\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 282s 629ms/step - loss: 1.3500 - accuracy: 0.4873 - val_loss: 1.3171 - val_accuracy: 0.4993\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 283s 631ms/step - loss: 1.3059 - accuracy: 0.5068 - val_loss: 1.2747 - val_accuracy: 0.5201\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 282s 630ms/step - loss: 1.2668 - accuracy: 0.5205 - val_loss: 1.2448 - val_accuracy: 0.5314\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 283s 632ms/step - loss: 1.2345 - accuracy: 0.5334 - val_loss: 1.2274 - val_accuracy: 0.5322\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 284s 634ms/step - loss: 1.2040 - accuracy: 0.5447 - val_loss: 1.1958 - val_accuracy: 0.5508\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 284s 634ms/step - loss: 1.1777 - accuracy: 0.5555 - val_loss: 1.1781 - val_accuracy: 0.5586\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 284s 633ms/step - loss: 1.1548 - accuracy: 0.5672 - val_loss: 1.1681 - val_accuracy: 0.5558\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 289s 645ms/step - loss: 1.1226 - accuracy: 0.5796 - val_loss: 1.1409 - val_accuracy: 0.5689\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 119290s 267s/step - loss: 1.1001 - accuracy: 0.5861 - val_loss: 1.1337 - val_accuracy: 0.5745\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 362s 807ms/step - loss: 1.0809 - accuracy: 0.5967 - val_loss: 1.1266 - val_accuracy: 0.5734\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 342s 764ms/step - loss: 1.0557 - accuracy: 0.6078 - val_loss: 1.1094 - val_accuracy: 0.5833\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 399s 890ms/step - loss: 1.0344 - accuracy: 0.6151 - val_loss: 1.1014 - val_accuracy: 0.5845\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 350s 781ms/step - loss: 1.0130 - accuracy: 0.6261 - val_loss: 1.0924 - val_accuracy: 0.5898\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 342s 762ms/step - loss: 0.9908 - accuracy: 0.6314 - val_loss: 1.1215 - val_accuracy: 0.5809\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 316s 704ms/step - loss: 0.9694 - accuracy: 0.6413 - val_loss: 1.0829 - val_accuracy: 0.5919\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 305s 680ms/step - loss: 0.9437 - accuracy: 0.6517 - val_loss: 1.0776 - val_accuracy: 0.6002\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 282s 629ms/step - loss: 0.9215 - accuracy: 0.6619 - val_loss: 1.0781 - val_accuracy: 0.5989\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 249s 556ms/step - loss: 0.9057 - accuracy: 0.6692 - val_loss: 1.0698 - val_accuracy: 0.6060\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 228s 509ms/step - loss: 0.8800 - accuracy: 0.6761 - val_loss: 1.0709 - val_accuracy: 0.6037\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 221s 494ms/step - loss: 0.8595 - accuracy: 0.6827 - val_loss: 1.0646 - val_accuracy: 0.6084\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 221s 494ms/step - loss: 0.8340 - accuracy: 0.6942 - val_loss: 1.0694 - val_accuracy: 0.6087\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 219s 488ms/step - loss: 0.8132 - accuracy: 0.7045 - val_loss: 1.0560 - val_accuracy: 0.6113\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 218s 486ms/step - loss: 0.7909 - accuracy: 0.7117 - val_loss: 1.0591 - val_accuracy: 0.6120\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 217s 484ms/step - loss: 0.7793 - accuracy: 0.7142 - val_loss: 1.0562 - val_accuracy: 0.6102\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 217s 485ms/step - loss: 0.7446 - accuracy: 0.7252 - val_loss: 1.0597 - val_accuracy: 0.6140\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 217s 484ms/step - loss: 0.7230 - accuracy: 0.7373 - val_loss: 1.0697 - val_accuracy: 0.6158\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 218s 487ms/step - loss: 0.7061 - accuracy: 0.7428 - val_loss: 1.0649 - val_accuracy: 0.6155\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 217s 484ms/step - loss: 0.6855 - accuracy: 0.7528 - val_loss: 1.0703 - val_accuracy: 0.6176\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 218s 486ms/step - loss: 0.6702 - accuracy: 0.7528 - val_loss: 1.0644 - val_accuracy: 0.6196\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 217s 485ms/step - loss: 0.6460 - accuracy: 0.7663 - val_loss: 1.0799 - val_accuracy: 0.6204\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 217s 484ms/step - loss: 0.6287 - accuracy: 0.7735 - val_loss: 1.0727 - val_accuracy: 0.6251\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 216s 482ms/step - loss: 0.6049 - accuracy: 0.7810 - val_loss: 1.0840 - val_accuracy: 0.6196\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 217s 484ms/step - loss: 0.5867 - accuracy: 0.7870 - val_loss: 1.0831 - val_accuracy: 0.6247\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 217s 484ms/step - loss: 0.5697 - accuracy: 0.7948 - val_loss: 1.1178 - val_accuracy: 0.6183\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 217s 485ms/step - loss: 0.5502 - accuracy: 0.8008 - val_loss: 1.1079 - val_accuracy: 0.6222\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 216s 482ms/step - loss: 0.5319 - accuracy: 0.8064 - val_loss: 1.0975 - val_accuracy: 0.6228\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 217s 484ms/step - loss: 0.5149 - accuracy: 0.8126 - val_loss: 1.1137 - val_accuracy: 0.6223\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 218s 487ms/step - loss: 0.5049 - accuracy: 0.8172 - val_loss: 1.1146 - val_accuracy: 0.6235\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 218s 487ms/step - loss: 0.4876 - accuracy: 0.8242 - val_loss: 1.1347 - val_accuracy: 0.6226\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 217s 484ms/step - loss: 0.4665 - accuracy: 0.8334 - val_loss: 1.1363 - val_accuracy: 0.6223\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 216s 483ms/step - loss: 0.4546 - accuracy: 0.8363 - val_loss: 1.1505 - val_accuracy: 0.6264\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 217s 483ms/step - loss: 0.4490 - accuracy: 0.8368 - val_loss: 1.1543 - val_accuracy: 0.6229\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 217s 485ms/step - loss: 0.4206 - accuracy: 0.8492 - val_loss: 1.1571 - val_accuracy: 0.6240\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 216s 483ms/step - loss: 0.4134 - accuracy: 0.8503 - val_loss: 1.1496 - val_accuracy: 0.6288\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 216s 482ms/step - loss: 0.3994 - accuracy: 0.8554 - val_loss: 1.1918 - val_accuracy: 0.6278\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'COLOR_BGR2gray_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     63\u001b[0m bounding_box \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mCascadeClassifier(\u001b[39m'\u001b[39m\u001b[39mhaarcascade_frontalface_default.xml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m gray_frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2gray_frame)\n\u001b[0;32m     65\u001b[0m num_faces \u001b[39m=\u001b[39m bounding_box\u001b[39m.\u001b[39mdetectMultiScale(gray_frame,scaleFactor\u001b[39m=\u001b[39m\u001b[39m1.3\u001b[39m, minNeighbors\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m (x, y, w, h) \u001b[39min\u001b[39;00m num_faces:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'COLOR_BGR2gray_frame'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir='data/train'\n",
    "val_dir='data/test'\n",
    "train_datagen=ImageDataGenerator(rescale=1./255)\n",
    "val_datagen=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48,48),\n",
    "    batch_size=64,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical')\n",
    "validation_generator=val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(48,48),\n",
    "    batch_size=64,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical'\n",
    ")\n",
    "#cnn\n",
    "emotion_model=Sequential()\n",
    "emotion_model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emotion_model.add(Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024,activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7,activation='softmax'))\n",
    "\n",
    "#compile and train\n",
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001,decay=1e-6),metrics=['accuracy'])\n",
    "emotion_model_info=emotion_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=28709//64,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=7178//64\n",
    ")\n",
    "#save model weights\n",
    "emotion_model.save_weights('model.h5')\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    bounding_box = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2gray_frame)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "       cap.release()\n",
    "       cv2.destroyAllWindows()\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
